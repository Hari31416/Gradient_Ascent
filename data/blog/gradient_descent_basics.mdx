---
title: 'Gradient Descent'
date: '2024-10-12'
tags: ['ml', 'maths', 'how-to']
draft: false
summary: 'A beginner-friendly guide to understanding gradient descent, a fundamental optimization algorithm used in machine learning.'
images: ['/static/images/gradient_descent_basics/banner.png']
layout: PostLayout
---

<TOCInline toc={props.toc} exclude="Introduction" />

# Intoduction

Gradient Descent (GD) is a fundamental optimization algorithm used in machine learning to minimize a function. It is used to find the minimum of a function by iteratively moving in the direction of the steepest descent. GD is the _secret sauce_ behind the success of many machine learning algorithms and neural networks.

GD works by taking small steps in the direction of the negative gradient of the function. Mathematically, it can be expressed as:

$$
\theta_{t+1} = \theta_t - \alpha \nabla f(\theta_t)
$$

To understand what this equation and the various terms mean, we have to start from the basics.

# Preliminary

## Function

## Derivative and Gradient

# Requirements for GD

## Differentiablity

## Convexity

# Examples

## Example 1 (Convex)

## Example 2 (Semi Convex)

# Limitations and Challenges

# Summary

# Sources
